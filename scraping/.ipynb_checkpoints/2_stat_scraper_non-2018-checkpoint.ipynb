{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Stat Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I originally worked on this project, I only scraped data for the regular season of 2018 and part of 2019. The remainder of 2018 was finished in another Notebook. The below code scrapes all missing data from 2013 - 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "#SQL Imports\n",
    "import mysql.connector\n",
    "#Pandas imports\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMySQLInterfaceError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mysql/connector/connection_cext.py\u001b[0m in \u001b[0;36m_open_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmysql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcnx_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMySQLInterfaceError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMySQLInterfaceError\u001b[0m: Access denied for user 'root'@'localhost' (using password: NO)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bd2d1b1ba4f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mpasswd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NRL_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0mmycursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmydb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mysql/connector/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mHAVE_CEXT\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_pure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCMySQLConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mMySQLConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0mConnect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnect\u001b[0m  \u001b[0;31m# pylint: disable=C0103\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mysql/connector/connection_cext.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mysql/connector/abstracts.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mysql/connector/connection_cext.py\u001b[0m in \u001b[0;36m_open_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMySQLInterfaceError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             raise errors.get_mysql_exception(msg=exc.msg, errno=exc.errno,\n\u001b[0;32m--> 182\u001b[0;31m                                              sqlstate=exc.sqlstate)\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProgrammingError\u001b[0m: 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)"
     ]
    }
   ],
   "source": [
    "#DB Connection\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  passwd=\"\",\n",
    "  database=\"NRL_data\"\n",
    ")\n",
    "mycursor = mydb.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get URL's of all matches that have not been scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>home_team_id</th>\n",
       "      <th>away_team_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>1467</td>\n",
       "      <td>2013-09-13</td>\n",
       "      <td>https://www.nrl.com/draw/nrl-premiership/2013/...</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>1468</td>\n",
       "      <td>2013-09-14</td>\n",
       "      <td>https://www.nrl.com/draw/nrl-premiership/2013/...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>1469</td>\n",
       "      <td>2013-09-14</td>\n",
       "      <td>https://www.nrl.com/draw/nrl-premiership/2013/...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>1470</td>\n",
       "      <td>2013-09-15</td>\n",
       "      <td>https://www.nrl.com/draw/nrl-premiership/2013/...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>1471</td>\n",
       "      <td>2013-09-20</td>\n",
       "      <td>https://www.nrl.com/draw/nrl-premiership/2013/...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>1472</td>\n",
       "      <td>2013-09-21</td>\n",
       "      <td>https://www.nrl.com/draw/nrl-premiership/2013/...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>1473</td>\n",
       "      <td>2013-09-27</td>\n",
       "      <td>https://www.nrl.com/draw/nrl-premiership/2013/...</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>1474</td>\n",
       "      <td>2013-09-28</td>\n",
       "      <td>https://www.nrl.com/draw/nrl-premiership/2013/...</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>1475</td>\n",
       "      <td>2013-10-06</td>\n",
       "      <td>https://www.nrl.com/draw/nrl-premiership/2013/...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id        date                                                url  \\\n",
       "1398  1467  2013-09-13  https://www.nrl.com/draw/nrl-premiership/2013/...   \n",
       "1399  1468  2013-09-14  https://www.nrl.com/draw/nrl-premiership/2013/...   \n",
       "1400  1469  2013-09-14  https://www.nrl.com/draw/nrl-premiership/2013/...   \n",
       "1401  1470  2013-09-15  https://www.nrl.com/draw/nrl-premiership/2013/...   \n",
       "1402  1471  2013-09-20  https://www.nrl.com/draw/nrl-premiership/2013/...   \n",
       "1403  1472  2013-09-21  https://www.nrl.com/draw/nrl-premiership/2013/...   \n",
       "1404  1473  2013-09-27  https://www.nrl.com/draw/nrl-premiership/2013/...   \n",
       "1405  1474  2013-09-28  https://www.nrl.com/draw/nrl-premiership/2013/...   \n",
       "1406  1475  2013-10-06  https://www.nrl.com/draw/nrl-premiership/2013/...   \n",
       "\n",
       "      home_team_id  away_team_id  \n",
       "1398            13             7  \n",
       "1399             4            10  \n",
       "1400            15             6  \n",
       "1401             3             8  \n",
       "1402             6             4  \n",
       "1403             7             8  \n",
       "1404            13             6  \n",
       "1405            15             8  \n",
       "1406            15             6  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all matches\n",
    "all_matches_query = pd.read_sql_query('SELECT id, date, url, home_team_id, away_team_id FROM Matches;', mydb)\n",
    "all_match_df = pd.DataFrame(all_matches_query, columns=['id', 'date', 'url', 'home_team_id', 'away_team_id'])\n",
    "\n",
    "#Find matches that were already scraped\n",
    "already_scraped = 'SELECT DISTINCT match_id FROM PlayerMatchStats;'\n",
    "mycursor.execute(already_scraped,)\n",
    "results = mycursor.fetchall()\n",
    "already_scraped_list = list(map(lambda x: x[0], results))\n",
    "\n",
    "#Remove matches which were already scraped and save remaining match info to not_yet_scraped_df\n",
    "not_yet_scraped = set(list(all_match_df['id'])) - set(already_scraped_list)\n",
    "not_yet_scraped_df = all_match_df[all_match_df['id'].isin(not_yet_scraped)]\n",
    "not_yet_scraped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds team id based off nickname\n",
    "def find_team_id(name):\n",
    "    find_team_query = 'SELECT id FROM Teams WHERE nickname = %s;'\n",
    "    mycursor.execute(find_team_query, (name,))\n",
    "    return mycursor.fetchone()[0]\n",
    "\n",
    "#Finds player based off name or creates entry in database for player\n",
    "def find_or_create_player(first_name, last_name, team_id):\n",
    "    find_player_query = 'SELECT id FROM Players WHERE first_name = %s AND last_name LIKE %s AND current_team = %s LIMIT 1;'\n",
    "    mycursor.execute(find_player_query, (first_name, '%' + last_name + '%', team_id))\n",
    "    result = mycursor.fetchone()\n",
    "    if result is None:\n",
    "        insert_player_query = 'INSERT INTO Players (first_name, last_name, current_team) VALUES (%s, %s, %s);'\n",
    "        data = (first_name, last_name, team_id)\n",
    "        mycursor.execute(insert_player_query, data)\n",
    "        mydb.commit()\n",
    "        result = find_or_create_player(first_name, last_name, team_id)\n",
    "        return int(result)\n",
    "    else:\n",
    "        result = result[0]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to scrape each match\n",
    "def scrape_match(match, web_driver):\n",
    "    match_key = match['url'].split('nrl-premiership/')[1][:-1]\n",
    "    for char in ['-vs-', '-v-', '/', '-']:\n",
    "        match_key = match_key.replace(char, '_')\n",
    "    results = {}\n",
    "    \n",
    "    #Use Selenium WebDriver to scrape data and automate the process of flipping through URLs\n",
    "    web_driver.get(match['url'])\n",
    "    for xpath in ['1', '2']:\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"player-stats\"]/div[' + xpath + ']/div/div[3]/div/table/tbody')))\n",
    "        for i in range(1, 20):\n",
    "            try:\n",
    "                name_field = driver.find_element_by_xpath('//*[@id=\"player-stats\"]/div[' + xpath + ']/div/div[3]/div/table/tbody/tr['+ str(i) +']/td[2]/a').get_attribute('innerText').strip()\n",
    "                first_name = name_field.split(' ')[0].strip().capitalize()\n",
    "                last_name = name_field.split(' ')[-1].strip().capitalize()\n",
    "                middle_name = name_field.split(' ')[-2].strip()\n",
    "                if middle_name.isalpha():\n",
    "                    last_name = middle_name.capitalize() + ' ' + last_name\n",
    "                if xpath == '1':\n",
    "                    team_id = match['home_team_id']\n",
    "                elif xpath == '2':\n",
    "                    team_id = match['away_team_id']\n",
    "                full_name = first_name + '_' + last_name + '_' + str(team_id)\n",
    "                player_id = find_or_create_player(first_name, last_name, str(team_id))\n",
    "                    \n",
    "                player_stat_list = []\n",
    "                player_stat_list.append(player_id)\n",
    "                player_stat_list.append(team_id)\n",
    "                player_stat_list.append(match['id'])\n",
    "                    \n",
    "                for column in range(3, 67):\n",
    "                    if column in [5, 7, 15, 17, 21, 34, 40, 47, 56, 64]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        stat_field = driver.find_element_by_xpath('//*[@id=\"player-stats\"]/div[' + xpath + ']/div/div[3]/div/table/tbody/tr[' + str(i) + ']/td[' + str(column) + ']')\n",
    "                        player_stat_list.append(stat_field.get_attribute('innerText').strip())\n",
    "                print(player_stat_list)\n",
    "                results[full_name] = player_stat_list\n",
    "            except:\n",
    "                continue\n",
    "    print('Scraping Success: ' + match['url'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to save each match's data to a CSV for insertion into the DB\n",
    "def save_data_to_csv(match, match_dict):\n",
    "    column_names = ['player_id', 'team_id', 'match_id', 'number', 'position', 'minutes_played', 'points', 'tries',\n",
    "                'conversions','conversion_attempts', 'penalty_goals', 'conversion_percentage','field_goals',\n",
    "                'fantasy_points', 'total_runs', 'total_run_metres', 'kick_return_metres', 'post_contact_metres',\n",
    "                'line_breaks', 'line_break_assists', 'try_assists', 'line_engaged_runs', 'tackle_breaks', 'hit_ups',\n",
    "                'play_the_ball', 'average_play_the_ball_seconds', 'dummy_half_runs', 'dummy_half_run_metres', \n",
    "                'steals', 'offloads', 'dummy_passes', 'passes', 'receipts', 'pass_to_run_ratio', 'tackle_percentage',\n",
    "                'tackles_made', 'tackles_missed', 'ineffective_tackles', 'intercepts', 'kicks_defused', 'kicks',\n",
    "                'kicking_metres', 'forced_drop_outs', 'bomb_kicks', 'grubbers', 'fourty_twenty',\n",
    "                'cross_field_kicks', 'kicked_dead', 'errors', 'handling_errors', 'one_on_ones_lost', 'penalties',\n",
    "                'on_report', 'sin_bins', 'send_offs', 'stint_one', 'stint_two']\n",
    "    year = match['date'].year\n",
    "    month = match['date'].month\n",
    "    match_id = match['id']\n",
    "    csv_filename = str(year) + '_' + str(month) + '_' + 'MatchID_' + str(match_id) + '.csv'\n",
    "    #print(csv_filename)\n",
    "    \n",
    "    csv_data = pd.DataFrame.from_dict(match_dict, orient='index', columns=column_names).reset_index()\n",
    "    print(csv_data)\n",
    "    csv_data = csv_data.replace('-', 0).replace({pd.np.nan: 0})\n",
    "    \n",
    "    yeardir = './csv_files/' + str(year)\n",
    "    if not os.path.exists(yeardir):\n",
    "        os.mkdir(yeardir)\n",
    "    monthdir = yeardir + '/' + str(month)\n",
    "    if not os.path.exists(monthdir):\n",
    "        os.mkdir(monthdir)\n",
    "    csv_data.to_csv(monthdir + '/' + csv_filename)\n",
    "    print(\"CSV success: \" + str(match_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scraping Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Up WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "wait = WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_errors = []\n",
    "csv_conversion_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in not_yet_scraped_df.iterrows():\n",
    "    try:\n",
    "        \n",
    "        #For each match, run the scraping function from step 2\n",
    "        match_data = scrape_match(match[1], driver)\n",
    "        try:\n",
    "            #If successful, run the save_data_to_csv function from step 2\n",
    "            save_data_to_csv(match[1], match_data)\n",
    "        \n",
    "        except:\n",
    "            #If unsuccessful, save URL to a CSV conversion error list\n",
    "            print(\"CSV error: \" + str(match[1]['id']))\n",
    "            csv_conversion_errors.append(match[1]['id'])\n",
    "    \n",
    "    except:\n",
    "        #If the scraping function fails at any step, save URL to scraping_error list\n",
    "        print('scraping error: ' + str(match[1]['url']))\n",
    "        scraping_errors.append(match[1]['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Update URLs for matches which could not be scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1467, 'https://www.nrl.com/draw/nrl-premiership/2013/finals-week-1/rabbitohs-vs-storm/'), (1468, 'https://www.nrl.com/draw/nrl-premiership/2013/finals-week-1/sharks-vs-cowboys/'), (1469, 'https://www.nrl.com/draw/nrl-premiership/2013/finals-week-1/roosters-vs-sea-eagles/'), (1470, 'https://www.nrl.com/draw/nrl-premiership/2013/finals-week-1/bulldogs-vs-knights/'), (1471, 'https://www.nrl.com/draw/nrl-premiership/2013/finals-week-2/sea-eagles-vs-sharks/'), (1472, 'https://www.nrl.com/draw/nrl-premiership/2013/finals-week-2/storm-vs-knights/'), (1473, 'https://www.nrl.com/draw/nrl-premiership/2013/finals-week-3/rabbitohs-vs-sea-eagles/'), (1474, 'https://www.nrl.com/draw/nrl-premiership/2013/finals-week-3/roosters-vs-knights/'), (1475, 'https://www.nrl.com/draw/nrl-premiership/2013/grand-final/roosters-vs-sea-eagles/')]\n"
     ]
    }
   ],
   "source": [
    "#Get match ids of errored URLs for manual updating\n",
    "find_incorrect_url_matches = 'SELECT id, url FROM Matches WHERE url IN {};'.format(tuple(scraping_errors))\n",
    "mycursor.execute(find_incorrect_url_matches,)\n",
    "results = mycursor.fetchall()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually update URLs which could not be scraped\n",
    "mycursor.executemany(\"UPDATE Matches SET url = %s WHERE id = %s\",\n",
    "                    [(\"https://www.nrl.com/draw/nrl-premiership/2013/round-27/rabbitohs-vs-storm/\", \"1467\"),\n",
    "                    (\"https://www.nrl.com/draw/nrl-premiership/2013/round-27/sharks-vs-cowboys/\", \"1468\"),\n",
    "                    (\"https://www.nrl.com/draw/nrl-premiership/2013/round-27/roosters-vs-sea-eagles/\", \"1469\"),\n",
    "                    (\"https://www.nrl.com/draw/nrl-premiership/2013/round-27/knights-vs-bulldogs/\", \"1470\"),\n",
    "                    (\"https://www.nrl.com/draw/nrl-premiership/2013/round-28/sea-eagles-vs-sharks/\", \"1471\"),\n",
    "                    (\"https://www.nrl.com/draw/nrl-premiership/2013/round-28/storm-vs-knights/\", \"1472\"),\n",
    "                    (\"https://www.nrl.com/draw/nrl-premiership/2013/round-29/rabbitohs-vs-sea-eagles/\", \"1473\"),\n",
    "                    (\"https://www.nrl.com/draw/nrl-premiership/2013/round-29/roosters-vs-knights/\", \"1474\"),\n",
    "                    (\"https://www.nrl.com/draw/nrl-premiership/2013/round-30/roosters-vs-sea-eagles/\", \"1475\")])\n",
    "mydb.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Repeat process from step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several iterations, I successfully created CSV files of data for each match. The two issues that caused errors were:\n",
    "1. Incorrerct URL -> Resolution was to update the URL\n",
    "2. Selenium timeouot error on the URL -> Resolution was to simply re-scrape the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycursor.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
